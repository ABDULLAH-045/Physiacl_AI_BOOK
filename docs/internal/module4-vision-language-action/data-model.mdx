---
displayed_sidebar: null
---

# Data Model: Module 4: Vision-Language-Action (VLA)

This document defines the key data entities, communication patterns, and data structures relevant to the Vision-Language-Action (VLA) pipeline for humanoid robotics.

## 1. Key Data Entities

-   **Voice Command**: Raw audio input from the user.
-   **Text Command**: Transcribed natural language command (e.g., "pick up the red cup").
-   **Cognitive Plan**: A sequence of high-level action primitives generated by an LLM (e.g., `[find("red cup"), navigate_to("red cup"), pick_up("red cup"), deliver_to("user")]`).
-   **Action Primitive**: A predefined, low-level robot skill (e.g., `pick_up(object)`, `navigate_to(location)`, `detect(object)`).
-   **Robot State**: Current joint positions, velocities, link poses, sensor readings.
-   **Environment State**: Dynamic representation of the robot's surroundings, including object locations, properties, and obstacles (derived from vision).
-   **Vision Data**: Sensor data from cameras (RGB-D), LiDAR, etc., processed to identify objects, their properties, and locations.
-   **Audio Stream**: Input from microphones for speech recognition.

## 2. Communication Protocols & Data Structures

Communication within the VLA pipeline primarily leverages ROS 2 topics, services, and actions.

### ROS 2 Message Types (Examples)

-   **Audio Input**:
    -   `audio_common_msgs/AudioData`: For raw audio streams from microphones to the Whisper node.
-   **Text Commands**:
    -   `std_msgs/String`: For transcribed text commands from Whisper to the LLM planner.
-   **Cognitive Plans**:
    -   Custom message type (e.g., `vla_msgs/CognitivePlan`): A list of action primitives, potentially with parameters.
        ```
        # vla_msgs/msg/CognitivePlan.msg
        std_msgs/String[] action_primitives
        ```
-   **Action Primitives (to Action Executor)**:
    -   `std_msgs/String`: Simple string commands (e.g., "pick_up red_cup").
    -   Custom message types (e.g., `vla_msgs/ActionPrimitive`): Structuring commands with arguments.
        ```
        # vla_msgs/msg/ActionPrimitive.msg
        std_msgs/String name
        std_msgs/String[] args
        ```
-   **Vision Data**:
    -   `sensor_msgs/Image`, `sensor_msgs/PointCloud2`: Input for object detection, pose estimation from perception modules.
-   **Robot Control**:
    -   `geometry_msgs/Twist`: For navigation.
    -   `trajectory_msgs/JointTrajectory`: For manipulation.

### VLA Pipeline Stages & Data Flow

1.  **Speech-to-Text (Whisper Node)**:
    -   **Input**: `audio_common_msgs/AudioData` (from microphone).
    -   **Output**: `std_msgs/String` (transcribed text) on a topic like `/vla/text_command`.

2.  **LLM Planner Node**:
    -   **Input**: `std_msgs/String` (text command) on `/vla/text_command`.
    -   **Output**: `vla_msgs/CognitivePlan` (sequence of action primitives) on a topic like `/vla/cognitive_plan`.
    -   **Feedback**: May subscribe to robot state/perception results for re-planning.

3.  **Vision Perception Modules**:
    -   **Input**: `sensor_msgs/Image`, `sensor_msgs/PointCloud2` (from robot sensors/sim).
    -   **Output**: Object detections (e.g., `vla_msgs/DetectedObjects`), object poses (`geometry_msgs/PoseStamped`).

4.  **Action Executor Node**:
    -   **Input**: `vla_msgs/CognitivePlan` on `/vla/cognitive_plan`.
    -   **Output**: Publishes to ROS 2 topics (`geometry_msgs/Twist`, `trajectory_msgs/JointTrajectory`) or calls ROS 2 services/actions to control the robot.
    -   **Feedback**: Reports success/failure of action primitives to the LLM planner.

## 3. Data Management

-   **Audio Recordings**: Temporary storage for voice commands for transcription.
-   **LLM Prompts/Responses**: Logging of prompts and generated plans for debugging and analysis.
-   **Robot Action Logs**: Detailed logs of executed action primitives, their outcomes, and any errors.
-   **Configuration**: All ROS 2 configurations (launch files, parameter files) are version-controlled.
